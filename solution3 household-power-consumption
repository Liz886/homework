# %%
import numpy as np
import pandas as pd

import torch
import torch.nn as nn
from torch.utils.data import DataLoader, TensorDataset
from sklearn.preprocessing import MinMaxScaler
import matplotlib.pyplot as plt
# %%
# load data
df = pd.read_csv('data//household_power_consumption.txt', sep = ";")
df.head()

# %%
# check the data
df.info()

# %%
df['datetime'] = pd.to_datetime(df['Date'] + " " + df['Time'])
df.drop(['Date', 'Time'], axis = 1, inplace = True)

df = df.replace('?', np.nan)
# handle missing values
df = df.dropna()
for col in df.columns:
    if col != 'datetime':
        df[col] = df[col].astype(float)

# %%
print("Start Date: ", df['datetime'].min())
print("End Date: ", df['datetime'].max())

# %%
# split training and test sets
# the prediction and test collections are separated over time
n = len(df)
train_end = int(n * 0.6)
valid_end = int(n * 0.8)
train = df.iloc[:train_end]
valid = df.iloc[train_end:valid_end]
test = df.iloc[valid_end:]

# %%
# data normalization
scaler = MinMaxScaler()
train_scaled = scaler.fit_transform(train.drop('datetime', axis=1))
valid_scaled = scaler.transform(valid.drop('datetime', axis=1))
test_scaled = scaler.transform(test.drop('datetime', axis=1))

# %%
# split X and y
def create_sequences(data, seq_length=24):
    xs, ys = [], []
    for i in range(len(data) - seq_length):
        x = data[i:i+seq_length]
        y = data[i+seq_length, 0]  
        xs.append(x)
        ys.append(y)
    return np.array(xs), np.array(ys)

seq_length = 24
train_X, train_y = create_sequences(train_scaled, seq_length)
valid_X, valid_y = create_sequences(valid_scaled, seq_length)
test_X, test_y = create_sequences(test_scaled, seq_length)
# %%
# convert to PyTorch tensors
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
train_X_tensor = torch.FloatTensor(train_X).to(device)
train_y_tensor = torch.FloatTensor(train_y).to(device)
valid_X_tensor = torch.FloatTensor(valid_X).to(device)
valid_y_tensor = torch.FloatTensor(valid_y).to(device)
test_X_tensor = torch.FloatTensor(test_X).to(device)
test_y_tensor = torch.FloatTensor(test_y).to(device)

# %%
# creat dataloaders
batch_size = 64
train_dataset = TensorDataset(train_X_tensor, train_y_tensor)
valid_dataset = TensorDataset(valid_X_tensor, valid_y_tensor)
test_dataset = TensorDataset(test_X_tensor, test_y_tensor)
train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
valid_loader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False)
test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)

# %%
# build a LSTM model
class LSTMModel(nn.Module):
    def __init__(self, input_size, hidden_size=64, output_size=1):
        super(LSTMModel, self).__init__()
        self.lstm = nn.LSTM(input_size, hidden_size, batch_first=True)
        self.fc = nn.Linear(hidden_size, output_size)
    def forward(self, x):
        out, _ = self.lstm(x)
        out = self.fc(out[:, -1, :])
        return out.squeeze()

input_size = train_X.shape[2]
model = LSTMModel(input_size=input_size, hidden_size=64, output_size=1).to(device)
optimizer = torch.optim.Adam(model.parameters(), lr=0.001)
criterion = nn.MSELoss()
# %%
# train the model
epochs = 10
for epoch in range(epochs):
    model.train()
    train_loss = 0
    for batch_x, batch_y in train_loader:
        optimizer.zero_grad()
        output = model(batch_x)
        loss = criterion(output, batch_y)
        loss.backward()
        optimizer.step()
        train_loss += loss.item() * batch_x.size(0)
    avg_train_loss = train_loss / len(train_loader.dataset)
    print(f"Epoch {epoch+1}/{epochs}, Train Loss: {avg_train_loss:.6f}")
# %%
# evaluate the model on the test set
model.eval()
valid_loss = 0
with torch.no_grad():
    for batch_x, batch_y in valid_loader:
            output = model(batch_x)
            loss = criterion(output, batch_y)
            valid_loss += loss.item() * batch_x.size(0)
    avg_valid_loss = valid_loss / len(valid_loader.dataset)
    print(f"Epoch {epoch+1}/{epochs}, Train Loss: {avg_train_loss:.6f}, Valid Loss: {avg_valid_loss:.6f}")

# %%
# make predictons on the test set
model.eval()
preds = []
with torch.no_grad():
    for batch_x, _ in test_loader:
        pred = model(batch_x)
        preds.append(pred.cpu().numpy())
pred_y = np.concatenate(preds)

# %%
# inverse transform the predictions and ground truth
test_y_true = test_y
test_y_true_unscaled = scaler.inverse_transform(
    np.concatenate([test_y_true.reshape(-1,1), np.zeros((test_y_true.shape[0], train_X.shape[2]-1))], axis=1)
)[:,0]
pred_y_unscaled = scaler.inverse_transform(
    np.concatenate([pred_y.reshape(-1,1), np.zeros((pred_y.shape[0], train_X.shape[2]-1))], axis=1)
)[:,0]

# %%
# plotting the predictions against the ground truth
plt.figure(figsize=(12,6))
plt.plot(pred_y_unscaled, label='Prediction')
plt.plot(test_y_true_unscaled, label='Ground Truth')
plt.xlabel('Samples')
plt.ylabel('Global_active_power')
plt.title('Prediction vs Ground Truth')
plt.legend()
plt.show()